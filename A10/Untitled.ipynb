{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "suffering-malpractice",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "from sklearn.model_selection import train_test_split\n",
    "import platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "sudden-plenty",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "total_class = len(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "little-skill",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "n_inputs = 28 ### lengh of each row\n",
    "n_steps = 28 ### number of time steps\n",
    "n_layers = 3 ### number of BasicRNNCell layers\n",
    "n_neurons = 100 ### number of neurons in the network\n",
    "n_outputs = 10 ### outputs that represent digits from 0-9\n",
    "training_epochs = 100\n",
    "learning_rate = 0.001\n",
    "batch_size = 50\n",
    "patience = int(np.sqrt(training_epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "forbidden-pillow",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_graph(seed=42): ### it resests all created graph, it's required once re-defining of any placeholders, variables, shapes or model structures is needed\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "educated-aspect",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(x_train, y_train, batch_size): ### it takes random permutation of lenght x_train and splits x_train (together with y_train) into batches number\n",
    "    rnd_idx = np.random.permutation(len(x_train))\n",
    "    n_batches = len(x_train) // batch_size\n",
    "    for batch_idx in np.array_split(rnd_idx, n_batches):\n",
    "        x_batch, y_batch = x_train[batch_idx], y_train[batch_idx]\n",
    "        yield x_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "legitimate-huntington",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data() ### loading the datasets\n",
    "x_train = x_train.astype(np.float32).reshape(-1, 28*28) / 255.0 ### reshaping and normalizing\n",
    "x_test = x_test.astype(np.float32).reshape(-1, 28*28) / 255.0 ### reshaping and normalizing\n",
    "y_train = y_train.astype(np.int32)\n",
    "y_test = y_test.astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "german-millennium",
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "aging-anger",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Inputs/X:0\", shape=(?, 28, 28), dtype=float32)\n",
      "Tensor(\"Inputs/y:0\", shape=(?,), dtype=int32)\n",
      "Tensor(\"Inputs/keep_probability:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope(\"Inputs\"):\n",
    "    X = tf.placeholder(tf.float32, [None, n_steps, n_inputs], name=\"X\")\n",
    "    y = tf.placeholder(tf.int32, [None], name=\"y\")\n",
    "    keep_prob = tf.placeholder_with_default(1.0, shape=(), name='keep_probability')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "durable-motivation",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sauravkoranga/.local/lib/python3.8/site-packages/tensorflow/python/keras/layers/legacy_rnn/rnn_cell_impl.py:421: UserWarning: `tf.nn.rnn_cell.BasicRNNCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.SimpleRNNCell`, and will be replaced by that in Tensorflow 2.0.\n",
      "  warnings.warn(\"`tf.nn.rnn_cell.BasicRNNCell` is deprecated and will be \"\n",
      "/home/sauravkoranga/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer_v1.py:1727: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
      "  warnings.warn('`layer.add_variable` is deprecated and '\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope(\"Basic_RNN_Layers\"):\n",
    "    lstm_cells = [tf.nn.rnn_cell.BasicRNNCell(num_units = n_neurons, activation = tf.nn.relu)\n",
    "             for layer in range(n_layers)]\n",
    "    lstm_cells_drop = [tf.nn.rnn_cell.DropoutWrapper(cell, input_keep_prob=keep_prob)\n",
    "                for cell in lstm_cells]\n",
    "    multi_layer_cell = tf.nn.rnn_cell.MultiRNNCell(lstm_cells_drop)\n",
    "    outputs, states = tf.nn.dynamic_rnn(multi_layer_cell, X, dtype = tf.float32) ### states return final state (last output) of the multi_layer_cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "wired-pizza",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sauravkoranga/.local/lib/python3.8/site-packages/tensorflow/python/keras/legacy_tf_layers/core.py:171: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  warnings.warn('`tf.layers.dense` is deprecated and '\n",
      "/home/sauravkoranga/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer_v1.py:1719: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "  warnings.warn('`layer.apply` is deprecated and '\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope(\"Loss\"):\n",
    "    states_concat = tf.concat(axis=1, values=states, name='states_reshape')\n",
    "    dense1 = tf.layers.dense(states_concat, 64, name='dense_1')\n",
    "    dense2 = tf.layers.dense(dense1, 32, name='dense_2')\n",
    "    logits = tf.layers.dense(dense2, n_outputs, name='output_layer')\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=tf.reshape(logits, shape=(-1, n_outputs)), name='softmax_cross_entropy')\n",
    "    loss = tf.reduce_mean(xentropy, name='loss')\n",
    "    loss_summary = tf.summary.scalar('loss_summ', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "hydraulic-settlement",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"Train\"):    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.001, name='Adam_optimizer')\n",
    "    training_optimizer = optimizer.minimize(loss, name='training_Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "seventh-metadata",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"Evaluation\"):        \n",
    "    correct = tf.nn.in_top_k(tf.reshape(logits, (-1, n_outputs)), y, 1, name='inTopK')\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name='Accuracy')\n",
    "    accuracy_summary = tf.summary.scalar('Accuracy_Summ', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "tender-distance",
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "train_keep_prob = 0.8\n",
    "x_test = x_test.reshape((-1, n_steps, n_inputs)) ### reshaping test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "patent-school",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def log_dir(prefix=\"\"):\n",
    "    now = datetime.utcnow().strftime('%Y-%m-%d-%H-%m-%S')\n",
    "    root_logdir = \"TensorFlow_Logs\"\n",
    "    if prefix:\n",
    "        prefix += '-'\n",
    "    name = prefix + now\n",
    "    return '{}/{}/'.format(root_logdir, name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "streaming-cutting",
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir=log_dir(\"mnist_rnn_model\")\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "coordinate-freedom",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"./checkpoints/mnist_rnn_model.ckpt\"\n",
    "checkpoint_epoch_path = checkpoint_path + \".epoch\"\n",
    "final_model_path = \"./mnist_rnn_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "robust-basement",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    best_loss = np.infty                ### parameters for early stopping\n",
    "    epochs_without_progress = 0         ### once epochs_without_progress reaches the value\n",
    "    max_epochs_without_progress = 15    ### of max_epochs_without_progress, the model stops and saves last parameters\n",
    "\n",
    "\n",
    "    acc_list, acc_test_list, loss_list, loss_test_list = [], [], [], []\n",
    "    n_epochs = 400\n",
    "    batch_size = 128\n",
    "    with tf.Session() as sess:\n",
    "        init.run() \n",
    "        for epoch in range(n_epochs):\n",
    "            for x_batch, y_batch in generate_batch(x_train, y_train, batch_size):\n",
    "                x_batch = x_batch.reshape((-1, n_steps, n_inputs))\n",
    "\n",
    "                sess.run(training_optimizer, feed_dict={X: x_batch, y: y_batch, keep_prob: train_keep_prob})\n",
    "            acc_batch, loss_batch, acc_sum, loss_sum = sess.run([accuracy, loss, accuracy_summary, loss_summary], feed_dict={X: x_batch, y: y_batch, keep_prob: train_keep_prob})   \n",
    "   \n",
    "            acc_test, loss_test, acc_test_sum, loss_test_sum = sess.run([accuracy, loss, accuracy_summary, loss_summary], feed_dict={X: x_test, y: y_test})\n",
    "\n",
    "            acc_list.append(acc_batch)\n",
    "            loss_list.append(loss_batch)\n",
    "            acc_test_list.append(acc_test)\n",
    "            loss_test_list.append(loss_test)\n",
    "\n",
    "            file_writer.add_summary(acc_sum, epoch)\n",
    "            file_writer.add_summary(loss_sum, epoch)\n",
    "            file_writer.add_summary(acc_test_sum, epoch)\n",
    "            file_writer.add_summary(loss_test_sum, epoch)\n",
    "\n",
    "            if epoch % 5 == 0:\n",
    "                print(\"Epoch\", epoch,\n",
    "                      '\\tValidation accuracy: {:.3f}%'.format(acc_batch * 100), '\\tLoss: {:.3f}'.format(loss_batch))\n",
    "                saver.save(sess, checkpoint_path)\n",
    "                with open(checkpoint_epoch_path, \"wb\") as f:\n",
    "                    f.write(b'%d' % (epoch + 1))\n",
    "                if loss_batch < best_loss:\n",
    "                    saver.save(sess, final_model_path)\n",
    "                    best_loss = loss_batch\n",
    "                else:\n",
    "                    epochs_without_progress += 2\n",
    "                    if epochs_without_progress > max_epochs_without_progress:\n",
    "                        print('Early Stopping')\n",
    "                        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "incorporate-religion",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    best_loss = np.infty\n",
    "    epochs_without_progress = 0\n",
    "    max_epochs_without_progress = 15\n",
    "\n",
    "\n",
    "    acc_list, acc_test_list, loss_list, loss_test_list = [], [], [], []\n",
    "    n_epochs = 400\n",
    "    batch_size = 128\n",
    "    with tf.Session() as sess:\n",
    "        init.run() \n",
    "        for epoch in range(n_epochs):\n",
    "            for x_batch, y_batch in generate_batch(x_train, y_train, batch_size):\n",
    "                x_batch = x_batch.reshape((-1, n_steps, n_inputs))\n",
    "\n",
    "                sess.run(training_optimizer, feed_dict={X: x_batch, y: y_batch, keep_prob: train_keep_prob})\n",
    "            acc_batch, loss_batch, acc_sum, loss_sum = sess.run([accuracy, loss, accuracy_summary, loss_summary], feed_dict={X: x_batch, y: y_batch, keep_prob: train_keep_prob})   \n",
    "   \n",
    "            acc_test, loss_test, acc_test_sum, loss_test_sum = sess.run([accuracy, loss, accuracy_summary, loss_summary], feed_dict={X: x_test, y: y_test})\n",
    "\n",
    "            acc_list.append(acc_batch)\n",
    "            loss_list.append(loss_batch)\n",
    "            acc_test_list.append(acc_test)\n",
    "            loss_test_list.append(loss_test)  \n",
    "\n",
    "            file_writer.add_summary(acc_sum, epoch)\n",
    "            file_writer.add_summary(loss_sum, epoch)\n",
    "            file_writer.add_summary(acc_test_sum, epoch)\n",
    "            file_writer.add_summary(loss_test_sum, epoch)\n",
    "\n",
    "            if epoch % 5 == 0:\n",
    "                print(\"Epoch\", epoch, '\\tTest accuracy: {:.3f}%'.format(acc_test * 100), '\\tLoss: {:.3f}'.format(loss_batch))\n",
    "                saver.save(sess, checkpoint_path)\n",
    "                with open(checkpoint_epoch_path, \"wb\") as f:\n",
    "                    f.write(b'%d' % (epoch + 1))\n",
    "                if loss_batch < best_loss:\n",
    "                    saver.save(sess, final_model_path)\n",
    "                    best_loss = loss_batch\n",
    "                else:\n",
    "                    epochs_without_progress += 2\n",
    "                    if epochs_without_progress > max_epochs_without_progress:\n",
    "                        print('Early Stopping')\n",
    "                        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "olympic-jewel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 \tValidation accuracy: 82.812% \tLoss: 0.464\n",
      "Epoch 5 \tValidation accuracy: 86.719% \tLoss: 0.393\n",
      "Epoch 10 \tValidation accuracy: 82.812% \tLoss: 0.381\n",
      "Epoch 15 \tValidation accuracy: 96.094% \tLoss: 0.212\n",
      "Epoch 20 \tValidation accuracy: 89.062% \tLoss: 0.355\n",
      "Epoch 25 \tValidation accuracy: 87.500% \tLoss: 0.381\n",
      "Epoch 30 \tValidation accuracy: 93.750% \tLoss: 0.190\n",
      "Epoch 35 \tValidation accuracy: 91.406% \tLoss: 0.201\n",
      "Epoch 40 \tValidation accuracy: 92.969% \tLoss: 0.240\n",
      "Epoch 45 \tValidation accuracy: 87.500% \tLoss: 0.317\n",
      "Epoch 50 \tValidation accuracy: 91.406% \tLoss: 0.176\n",
      "Epoch 55 \tValidation accuracy: 90.625% \tLoss: 0.288\n",
      "Epoch 60 \tValidation accuracy: 91.406% \tLoss: 0.217\n",
      "Epoch 65 \tValidation accuracy: 84.375% \tLoss: 0.333\n",
      "Early Stopping\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acquired-editor",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
